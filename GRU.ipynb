{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight')\n",
    "from pylab import rcParams\n",
    "from plotly import tools\n",
    "import chart_studio.plotly as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import newaxis\n",
    "from keras.layers.core import Dense ,Activation,Dropout\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Input Variables\n",
    "index_reader = 1 #Input for Building\n",
    "no_of_epochs = 35 #Use only during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Making Files for building \n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'],infer_datetime_format=True)  \n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'],infer_datetime_format=True)  \n",
    "\n",
    "\n",
    "##Creating training files\n",
    "\n",
    "for i in range(1,6):\n",
    "    if(i==1):\n",
    "        df = train[train['building_number']==1]\n",
    "        df.index = df['timestamp']\n",
    "        df = df.resample('1H').mean()\n",
    "        new_df = df\n",
    "    else:\n",
    "        df = train[train['building_number']==i]\n",
    "        df.index = df['timestamp']\n",
    "        df = df.resample('1H').mean()\n",
    "        new_df = pd.concat([new_df,df])\n",
    "    \n",
    "#Adding new features\n",
    "new_df['day of week']=new_df.index.dayofweek \n",
    "new_df['Hour']=new_df.index.hour\n",
    "new_df['corporate'] = new_df['Hour'].apply(lambda x: 0 if 0<= x <= 7 or 20<=x<=23 else 1)\n",
    "new_df = new_df.drop(['Hour'],axis=1)\n",
    " \n",
    "anomaly_mean_main_meter = new_df['main_meter'].mean()\n",
    "new_df.loc[(new_df.main_meter > 15000),'main_meter']=anomaly_mean_main_meter\n",
    "anomaly_mean_sub_meter_1 = new_df['sub_meter_1'].mean()\n",
    "new_df.loc[(new_df.sub_meter_1 > 5000),'sub_meter_1']=anomaly_mean_sub_meter_1\n",
    "anomaly_mean_sub_meter_2 = new_df['sub_meter_2'].mean()\n",
    "new_df.loc[(new_df.sub_meter_2 > 3000),'sub_meter_2']=anomaly_mean_sub_meter_2\n",
    "\n",
    "one_hot = ['day of week','building_number']\n",
    "new_df = pd.get_dummies(new_df,columns = one_hot)\n",
    "\n",
    "#Saving training file\n",
    "for i in range(1,6):\n",
    "    new_df[new_df['building_number_'+str(i)]==1].to_csv('./csv_files/intermediate_files/building_'+str(i)+'_train.csv')\n",
    "\n",
    "##Creating testing files\n",
    "\n",
    "for i in range(1,6):\n",
    "    if(i==1):\n",
    "        df_test = test[test['building_number']==1]\n",
    "        df_test.index = df_test['timestamp']\n",
    "        df_test = df_test.resample('1H').mean()\n",
    "        new_df_test = df_test\n",
    "    else:\n",
    "        df_test = test[test['building_number']==i]\n",
    "        df_test.index = df_test['timestamp']\n",
    "        df_test = df_test.resample('1H').mean()\n",
    "        new_df_test = pd.concat([new_df_test,df_test])\n",
    "\n",
    "#Adding new features\n",
    "new_df_test['main_meter'] = 'NaN'\n",
    "new_df_test['sub_meter_1'] = 'NaN'\n",
    "new_df_test['sub_meter_2'] = 'NaN'\n",
    "new_df_test['day of week']=new_df_test.index.dayofweek\n",
    "new_df_test['Hour']=new_df_test.index.hour\n",
    "new_df_test['corporate'] = new_df_test['Hour'].apply(lambda x: 0 if 0<= x <= 7 or 20<=x<=23 else 1)\n",
    "new_df_test = new_df_test.drop(['Hour'],axis=1)\n",
    "\n",
    "one_hot = ['day of week','building_number']\n",
    "new_df_test = pd.get_dummies(new_df_test,columns = one_hot)\n",
    "\n",
    "#Saving test file\n",
    "for i in range(1,6):\n",
    "    new_df_test[new_df_test['building_number_'+str(i)]==1].to_csv('./csv_files/intermediate_files/building_'+str(i)+'_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Files\n",
    "dataframe = pd.read_csv('csv_files/intermediate_files/building_'+str(index_reader)+'_train.csv')\n",
    "shape_old_dataframe = dataframe.shape[0]\n",
    "test_building = pd.read_csv('csv_files/intermediate_files/building_'+str(index_reader)+'_test.csv')\n",
    "test_for_index = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defined Variables\n",
    "val_time = 1 #1 hour Aggregate\n",
    "n_input = 72 #Timestep size is 72\n",
    "n_features = 16 #Input features is 16\n",
    "val_length = test_building.shape[0] #Prediction file length for particular building\n",
    "n_pred = 3 \n",
    "val_length_new = int(float(shape_old_dataframe)*0.2)\n",
    "val_gap = val_time*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of raw data is -:  (9182, 17)\n",
      "Shape of training data -:  6600\n",
      "Shape of testing data -:  (2582, 17)\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing Dataframe\n",
    "dataframe = pd.concat([dataframe,test_building],axis=0)\n",
    "interval = [dataframe.shape[0]]\n",
    "print(\"The shape of raw data is -: \", dataframe.shape)\n",
    "print(\"Shape of training data -: \", shape_old_dataframe)\n",
    "print(\"Shape of testing data -: \", test_building.shape)\n",
    "dataframe['timestamp'] = pd.to_datetime(dataframe['timestamp'],infer_datetime_format=True)\n",
    "dataframe.set_index(dataframe['timestamp'],inplace=True)\n",
    "dataframe = dataframe.drop(['timestamp'],axis=1)\n",
    "#Taking particular features\n",
    "time_series = dataframe[['main_meter', 'sub_meter_1', 'sub_meter_2', 'corporate', 'day of week_0','day of week_1','day of week_2','day of week_3','day of week_4','day of week_5','day of week_6','building_number_1','building_number_2','building_number_3','building_number_4','building_number_5']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape  (6600, 16)\n",
      "Testing data shape  (2582, 16)\n",
      "Shape of training data after final processing -:  (6528, 72, 16)\n"
     ]
    }
   ],
   "source": [
    "#Splitting into training and test data\n",
    "sum = 0\n",
    "c = interval[0] \n",
    "train_data = time_series[:c-val_length]\n",
    "test_data = time_series[c-val_length:c]\n",
    "sum+=c\n",
    "interval[0] = sum -val_length\n",
    "\n",
    "test_data = pd.DataFrame(test_data).values.reshape(-1,n_features)\n",
    "train_data = pd.DataFrame(train_data).values.reshape(-1,n_features)\n",
    "print(\"Training data shape \", train_data.shape)\n",
    "print(\"Testing data shape \", test_data.shape)\n",
    "\n",
    "#Normalizing Dataframes\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(time_series[:interval[0]])\n",
    "scaled_train_data = scaler.transform(train_data)\n",
    "\n",
    "\n",
    "#Processing Training data and Testing Data for GRU\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(n_input, interval[0]):\n",
    "    X_train.append(scaled_train_data[i-n_input:i, :])\n",
    "    y_train.append(scaled_train_data[i, :n_pred])\n",
    "    \n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], n_input, n_features))\n",
    "\n",
    "perm = np.random.permutation(X_train.shape[0]) ##Shuffling data\n",
    "X_train = X_train[perm]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "print(\"Shape of training data after final processing -: \", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 72, 300)           286200    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 50)                52800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 339,153\n",
      "Trainable params: 339,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#Building GRU Model and Saving Model\n",
    "val = \"building\"+str(index_reader)+\"_1hr_sigmoid_3meter_adam_1_final\"\n",
    "\n",
    "###Comment out to run the Model\n",
    "'''\n",
    "gru_model = Sequential()\n",
    "gru_model.add(GRU(300,  return_sequences=True, activation='relu',input_shape=(n_input,n_features)))\n",
    "gru_model.add(GRU(50,activation='relu'))\n",
    "gru_model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "gru_model.compile(optimizer='adam',loss='mse')\n",
    "\n",
    "print(gru_model.summary())\n",
    "\n",
    "#Saving model\n",
    "model_json = gru_model.to_json()\n",
    "with open(\"./final_models/model_GRU_model_all_\"+ val +\".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "#Fitting Model\n",
    "gru_model.fit(X_train, y_train, epochs=no_of_epochs, batch_size = 1024)\n",
    "\n",
    "#Saving weights\n",
    "gru_model.save_weights(\"./final_weights/model_GRU_model_all_\"+ val +\".h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "'''\n",
    "#Loading Model\n",
    "json_file = open(\"./final_models/model_GRU_model_all_\"+ val +\".json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "gru_model = model_from_json(loaded_model_json)\n",
    "print(gru_model.summary())\n",
    "# load weights into new model\n",
    "gru_model.load_weights(\"./final_weights/model_GRU_model_all_\"+ val +\".h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting from Model\n",
    "gru_predictions_scaled = list()\n",
    "batch = scaled_train_data[interval[0]-n_input:interval[0], :]\n",
    "current_batch = batch.reshape((1,n_input,n_features))\n",
    "for i in range(0,val_length):   \n",
    "    gru_pred = ((gru_model.predict(current_batch)).astype(np.float64))\n",
    "    dummy = test_data[i].copy()\n",
    "    dummy[:n_pred] = gru_pred\n",
    "    gru_predictions_scaled.append(dummy)\n",
    "    current_batch = np.append(current_batch[:,1:],[dummy.reshape(1, -1)], axis=1)\n",
    "gru_predictions = scaler.inverse_transform(gru_predictions_scaled)\n",
    "gru_pred = np.array(gru_predictions[:, :n_pred])\n",
    "gru_pred = np.repeat(gru_pred, val_gap, axis = 0)\n",
    "final_gru_predictions = pd.DataFrame(gru_pred)\n",
    "final_gru_predictions.columns = ['main_meter','sub_meter_1','sub_meter_2']\n",
    "test_for_index = test_for_index[test_for_index['building_number']==index_reader]\n",
    "final_gru_predictions = final_gru_predictions.iloc[:test_for_index.shape[0],:]\n",
    "final_gru_predictions.index = test_for_index['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Dataframe into csv\n",
    "final_gru_predictions.to_csv('csv_files/prediction_files/GRU/building_'+str(index_reader)+'_3meter_gru_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
